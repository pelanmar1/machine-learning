x = load('ex4x.dat');y = load('ex4y.dat');[x_norm mu sigma] = featureNormalize(x);m = length(x);n = min(size(x))+1;X_norm = [ones(m,1) x_norm];X = [ones(m,1) x];theta = zeros(n,1);theta = [0, 0 , 0]';N = 10000;alpha = 0.01;tempJ = zeros(1,N);g = inline('1.0 ./ (1.0 + exp(-z))');       for i = 1:N    h = g(X_norm*theta);    theta = theta - alpha/m*(X_norm'*(h-y));    tempJ(i) =computeCost(X_norm,y,theta);endfprintf(['theta (With G.D.) = \n %f \n'], theta);% Plottingpos = find(y == 1);neg = find(y == 0);figure 1;subplot(2,2,[1 2]) plot(X_norm(pos, 2), X_norm(pos,3), '+b')hold onplot(X_norm(neg, 2), X_norm(neg, 3), 'or')xlabel('Exam 1 score')ylabel('Exam 2 score')plot_x = [min(X_norm(:,2))-2,  max(X_norm(:,2))+2];plot_y = (-1./theta(3)).*(theta(2).*plot_x +theta(1));plot(plot_x, plot_y)hold offtitle('Logistic Regression')subplot(2,2,3) plot(1:N,tempJ,'-b');xlabel('Iteration');ylabel('J')title('G.D. Convergence Rate')prediction = g([1 ([20 80]-mu)./sigma]*theta);fprintf(['P(Y=0|(x1=20,x2=80);theta) = \n %f \n'], 1-prediction);% By Newton's methodK = 7;theta = zeros(n,1);tempJN = zeros(1,K);for i = 1:K    theta = logRegNewton(X,y,theta);    tempJN(i) = computeCost(X,y,theta);endsubplot(2,2,4) plot(0:K-1, tempJN, 'o--', 'MarkerFaceColor', 'r', 'MarkerSize', 8)xlabel('Iteration');ylabel('J')title('Newtons Method Convergence Rate')fprintf(['theta (With Newton Method) = \n %f \n'], theta);prediction = g([1 20 80]*theta);fprintf(['P(Y=0|(x1=20,x2=80);theta) (With Newton Method) = \n %f \n'], 1-prediction);